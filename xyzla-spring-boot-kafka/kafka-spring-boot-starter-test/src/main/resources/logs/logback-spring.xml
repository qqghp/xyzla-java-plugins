<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- 日志级别 -->
    <property name="LOG_LEVEL" value="INFO"></property>
    <!-- 日志地址 -->
    <property name="LOG_HOME" value="./logs"/>
    <!-- 最大保存时间 -->
    <property name="MAX_HISTORY" value="30"/>
    <!-- 日志文件大小限制 -->
    <!-- <property name="MAX_FILE_SIZE" value="1GB"/>-->
    <property name="MAX_FILE_SIZE" value="1024MB"/>
    <!-- 异步缓冲队列的深度,该值会影响性能.默认值为256 -->
    <property name="QUEUE_SIZE" value="512"></property>

    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <Target>System.out</Target>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-4relative [%thread] %-5level %logger{35}:%line - %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="LOG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-4relative [%thread] %-5level %logger{35}:%line - %msg%n</pattern>
        </encoder>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>${LOG_LEVEL}</level>
        </filter>
        <file>${LOG_HOME}/application.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_HOME}/application.log.%d{yyyy-MM-dd}.%i.zip</fileNamePattern>
            <maxHistory>${MAX_HISTORY}</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>${MAX_FILE_SIZE}</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
    </appender>

    <!--错误-->
    <appender name="LOG_ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <file>${LOG_HOME}/kafka-error.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_HOME}/kafka-error.log.%d{yyyy-MM-dd}.%i.zip</fileNamePattern>
            <maxHistory>${MAX_HISTORY}</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>${MAX_FILE_SIZE}</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-4relative [%thread] %-5level %logger{35}:%line - %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="ASYNC_LOG_FILE" class="ch.qos.logback.classic.AsyncAppender">
        <!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 -->
        <discardingThreshold>0</discardingThreshold>
        <!-- 更改默认的队列的深度,该值会影响性能.默认值为256 -->
        <queueSize>${queueSize}</queueSize>
        <appender-ref ref="LOG_FILE"/>
    </appender>

    <appender name="ASYNC_LOG_ERROR" class="ch.qos.logback.classic.AsyncAppender">
        <!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志-->
        <discardingThreshold>0</discardingThreshold>
        <!--更改默认的队列的深度,该值会影响性能.默认值为256 -->
        <queueSize>${queueSize}</queueSize>
        <appender-ref ref="LOG_ERROR"/>
    </appender>

    <logger name="org.springframework" level="WARN"/>
    <logger name="ch.qos.logback" level="INFO"/>
    <logger name="org.apache.kafka" level="INFO"/>
    <logger name="io.undertow" level="INFO"></logger>
    <logger name="com.showmac" level="INFO"/>

    <logger name="java.sql" level="DEBUG">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="ASYNC_LOG_FILE"/>
    </logger>

    <root level="${LOG_LEVEL}">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="ASYNC_LOG_FILE"/>
        <appender-ref ref="ASYNC_LOG_ERROR"/>
    </root>
</configuration>